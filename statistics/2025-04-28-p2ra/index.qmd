---
title: "Read-based model for swab P2RA"
format:
  html:
    code-fold: true
    toc: true
  pdf:
    toc: true
editor: source 
author: dan
---

# Introduction

This document provides a statistical analysis approach for estimating virus prevalence from pooled nasal swab samples, and for calculating the relative abundance at 1% prevalence (RA_p(1%)) from wastewater metagenomic sequencing data. The approach addresses concerns raised in the internal memo regarding the sensitivity of detection in pooled samples with low read counts.

The key concerns being addressed include:

1. The possibility that low viral read counts in positive pools indicate that we're missing many true positive samples

2. The challenge of distinguishing between scenarios where we're detecting most positive samples versus scenarios where we're only detecting a fraction of them

3. The need for a statistical model that accounts for imperfect detection sensitivity

This analysis uses a Bayesian approach to jointly model both the prevalence data from swab samples and the relative abundance data from wastewater samples, allowing for uncertainty in detection.

## Setup

First, we'll load the necessary libraries and set up our analysis environment:

```{r setup}
library(tidyverse)
library(rstan)
library(knitr)
theme_set(theme_minimal())

# Colors borrowed from https://github.com/JLSteenwyk/ggpubfigs
wong_eight <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")
options(ggplot2.discrete.colour = function() scale_colour_manual(values = wong_eight))
options(ggplot2.discrete.fill = function() scale_fill_manual(values = wong_eight))

options(mc.cores = parallel::detectCores())
```

# Model

Our goal is to jointly estimate virus prevalence from pooled swab samples and relative abundance from wastewater samples, while:

1.  **Accounting for imperfect detection**: The model doesn't assume that all positive individuals will be detected. Instead, it models the probability of detecting various numbers of viral reads given the number of positive individuals in a pool.

2.  **Modeling the viral fraction directly**: The parameter `mu_1` represents the expected viral fraction in a pool containing a single positive swab. This can be very small, which would indicate low sensitivity.

3.  **Integrating over uncertainty**: Rather than assuming we know how many positive individuals are in each pool, the model integrates over all possibilities, weighted by their probability given the observed data.

4.  **Joint modeling with wastewater data**: By modeling both swab and wastewater data together, we can more naturally incorporate different sources of uncertainty.

## Parameters

-   $p$: Prevalence of the virus in the population (probability an individual is infected)
-   $\mu_1$: Expected viral fraction in a pool containing a single positive swab
-   $\phi_1$: Inverse-dispersion parameter for viral reads in a pool with a single positive swab
-   $\mu_{ww}$: Expected viral fraction in wastewater if everyone were infected
-   $\phi_{ww}$: Inverse-dispersion parameter for viral reads in wastewater

## Swab Sample Model

For each pool $i$ with $n_{swab,i}$ swabs:

1.  The number of positive swabs $n_{+,i}$ follows a binomial distribution: $$n_{+,i} \sim \text{Binomial}(n_{swab,i}, p)$$

2.  The viral fraction $v_i$ in the pool depends on the number of positive swabs: $$v_i \sim \text{Gamma}\left(n_{+,i}\phi_1, \frac{n_{swab,i}\phi_1}{\mu_1}\right)$$
    (in shape-rate parameterization)
    This gives: $$E[v_i] = \frac{n_{+,i}}{n_{swab,i}} \cdot \mu_1$$ $$\text{Var}[v_i] = \frac{n_{+,i}}{n_{swab,i}^2} \cdot \frac{\mu_1^2}{\phi_1}$$

3.  The viral read count follows a Poisson distribution conditional on the viral fraction: $$r_{viral,i} \mid v_i \sim \text{Poisson}(r_{total,i} \cdot v_i)$$

4.  Marginalizing over $v_i$, the viral read count follows a negative binomial distribution: $$r_{viral,i} \mid n_{+,i} \sim \text{NegBinom2}\left(\frac{n_{+,i} \cdot r_{total,i} \cdot \mu_1}{n_{swab,i}}, n_{+,i} \cdot \phi_1\right)$$

5.  Since we don't observe $n_{+,i}$, the likelihood for a pool is: $$P(r_{viral,i}) = \sum_{n_{+,i}=0}^{n_{swab,i}} P(r_{viral,i} \mid n_{+,i}) \cdot P(n_{+,i})$$

    Where:

    -   If $n_{+,i} = 0$, then $P(r_{viral,i} \mid n_{+,i} = 0) = \mathbf{1}[r_{viral,i} = 0]$
    -   Otherwise, $P(r_{viral,i} \mid n_{+,i})$ is given by the negative binomial above
    -   $P(n_{+,i})$ is given by the binomial distribution with parameters $n_{swab,i}$ and $p$

## Wastewater Sample Model

For each wastewater sample $j$:

1.  The expected viral read count depends on the prevalence: $$\mu_j = r_{total,j} \cdot p \cdot \mu_{ww}$$

2.  The viral read count follows a negative binomial distribution: $$r_{viral,j} \sim \text{NegBinom2}(\mu_j, \phi_{ww})$$

## Prior Distributions

```{r priors}
#| code-fold: false

### prevalence
alpha_p <- 0.5
beta_p <- 10
prior_p <- function(x) dbeta(x, alpha_p, beta_p)

### swab: logit(mu_1)
mean_logit_mu_1 <- -7  # Prior mean on logit scale
sd_logit_mu_1 <- 3     # Prior standard deviation on logit scale
prior_logit_mu_1 <- function(x) dnorm(x, mean_logit_mu_1, sd_logit_mu_1)

### swab: phi_1
shape_phi_1 <- 2
rate_phi_1 <- 2
prior_phi_1 <- function(x) dgamma(x, shape_phi_1, rate_phi_1)

### wastewater: logit(mu_ww)
mean_logit_mu_ww <- -11  # Prior mean on logit scale
sd_logit_mu_ww <- 3      # Prior standard deviation on logit scale
prior_logit_mu_ww <- function(x) dnorm(x, mean_logit_mu_ww, sd_logit_mu_ww)

### wastewater: phi_ww
shape_phi_ww <- 2
rate_phi_ww <- 2
prior_phi_ww <- function(x) dgamma(x, shape_phi_ww, rate_phi_ww)
```

### Prevalence

We want to constrain the prevalence to (0, 1) and we think it's unlikely that the prevalence of any virus is much greter than 10%. We don't have much prior information about the lower bound on prevalence, so we'll use an uninformative (Jeffreys) prior on the lower bound. This suggests:

$$p \sim \text{Beta}(0.5, 10)$$

```{r}
tibble(
  p = 10^seq(-5, 0, length.out = 1000),
  density = prior_p(p),
  density_log10 = density * p
) %>%
  ggplot(aes(p, density_log10)) +
  geom_line() +
  scale_x_log10()
```

This distribution has a mean of ~4.8% which Evan suggests might be high (though Katherine in all-hands seemed to think this might be reasonable).
Geometric mean is only 1.5% though and the distribution is looks reasonable to me in log space.

Looking at the posteriors below, all the posterior modes are shifted to the left relative to the prior, so we might want to consider adjusting accordingly.

### Expected viral fractions

For the viral fraction parameters (mu_1 and mu_ww), we use logit-normal priors to constrain these values between 0 and 1:

-   $\text{logit}(\mu_1) \sim \text{Normal}(-7, 3)$
-   $\text{logit}(\mu_{ww}) \sim \text{Normal}(-11, 3)$

Because we do not have much prior information about these parameters, we chose the hyperparameters, which set the mean and standard deviation of $\text{logit}(\mu)$, iteratively so that the priors only weakly constrain the posteriors in cases with observed reads.
Namely, we: (1) proposed some priors, (2) fit the model, (3) saw whether the priors were significantly constraining the posteriors for the cases where we have swab reads, (4) if yes, proposed broader priors.

We also included the following considerations:

-   Because we are looking at respiratory viruses, we expect a priori the viral fraction at a given prevalence to be higher in swabs than in wastewater.
-   We want the wastewater prior to be consistent with our previous P2RA estimates.
-   It seems unlikely that the expected viral fraction in a positive swab is O(1). We don't want to totally exclude this possibility, but a single very high count in a single pool seems more likely to be lucky than reflective of the average.

```{r}
crossing(datatype = factor(c("swab", "wastewater")),
         logit_mu = seq(-20, 4, length.out = 1000),) %>%
  mutate(
    mu = plogis(logit_mu),
    prior = if_else(
      datatype == "swab",
      prior_logit_mu_1(logit_mu),
      prior_logit_mu_ww(logit_mu)
    ),
  ) %>%
  ggplot(aes(mu, prior, color = datatype)) +
  geom_line() +
  scale_x_continuous(
    transform = "logit",
    breaks = c(10^seq(-8, -1, 1), 0.5, 0.9),
    name = "mu (logistic scale)"
  ) +
  labs(title = "Prior distributions on mu")
```

Note that previously, we tried Beta distributions for these priors.
We found that the Jeffrey's left tail ($\text{Beta}(\alpha = 1/2, \beta)$) put too little weight on small values.
We explored various smaller `\alpha` parameters, but found that Beta priors that put significant weight on values like 1e-6 were almost flat in log space, which puts too much weight on very small numbers and led to numerical instabilities.

### Overdispersion

Our final parameters are the inverse-overdispersion parameters for the read counts (phi_1 and phi_ww). A useful intuition for this parameter is that, as the read depth gets very large so that poisson counting noise becomes unimportant relative to other sources of variance, the coefficient of variation in read counts goes to $\phi^{-1/2}$.

We have very little prior information about this parameter except that it must be greater than zero, and we expect significant overdispersion in both sample sources. We also don't have strong reason to expect the overdispersion to be higher on one sample type than the other, so we'll put the same prior on both. (Swabs have noise from poor swabbing by subjects, but wastewater has all sorts of uncontrolled variables.)

Another complication is that we don't expect our data, especially the swab data, to give us much information about $\phi$ because you need multiple observations to estimate it well.

Here's a toy to get an order of magnitude expectation. Consider one contributing factor to overdispersion: the possiblity that some subjects swab their noses poorly and contribute little to the pool. Imagine that each individual either contributes no material to the pool or a fixed quantity. If the probability of getting a good swab is $x$, and this is the only source of overdispersion, then $CV^2 = \frac{1-x}{x}$. Therefore, $\phi = \frac{x}{1-x}$, the odds ratio of getting a good swab to getting a bad swab. If, e.g. 25% of swabs are poor, we'd expect $\phi \le 3$.

This toy model is likely an underestimate of the overdispersion because it only includes one specific source. That suggests that $\phi$ is not much larger than one.

With this in mind, we use priors:

-   $\phi_1 \sim \text{Gamma}(2, 2)$
-   $\phi_{ww} \sim \text{Gamma}(2, 2)$

which look like this on a log scale:

```{r}
crossing(
  datatype = factor(c("swab", "wastewater")),
  phi = seq(0.001, 10, 0.001)
) %>%
  mutate(
    prior = if_else(datatype == "swab", 
                          prior_phi_1(phi), 
                          prior_phi_ww(phi)),
    prior_log10 = prior * phi * log(10),
  ) %>%
  ggplot(aes(phi, prior, color = datatype, linetype = datatype)) +
  geom_line() +
  scale_x_log10() +
  labs(title = "Prior distributions on phi")
```

In the future, we may want to consider fitting a model to all viruses together and estimating a single pair of $\phi$ parameters for all viruses (or a hierarchical model that partially pools information). This could make sense because many of the sources of noise, including swab quality are probably independent of the virus.

## Stan model

Now we'll define the Stan model code.

```{r}
#| label: model
#| code-fold: false
#| cache: true

model_code = "
data {
  // Pooled swab sequence data
  int<lower=1> N_pools;                   // number of pools
  int<lower=1> n_swabs[N_pools];          // number of swabs in each pool
  int<lower=0> r_total_pool[N_pools];     // total read count
  int<lower=0> r_viral_pool[N_pools];     // viral read count
  
  // Wastewater MGS data
  int<lower=0> N_ww;                      // number of wastewater samples
  int<lower=0> r_total_ww[N_ww];          // total read count
  int<lower=0> r_viral_ww[N_ww];          // viral read count
}
  
parameters {
  real<lower=0, upper=1> p;       // prevalence
  
  // Swab parameters (using logit transformation)
  real logit_mu_1;                // logit of expected viral fraction in a pool containing a single positive swab
  real<lower=0> phi_1;            // inverse-dispersion in a pool containing a single positive swab
  
  // Wastewater MGS parameters (using logit transformation)
  real logit_mu_ww;               // logit of expected viral fraction in wastewater if everyone were infected
  real<lower=0> phi_ww;           // inverse-dispersion in wastewater MGS
}

transformed parameters {
  real<lower=0, upper=1> mu_1 = inv_logit(logit_mu_1);    // back-transform to [0,1] scale
  real<lower=0, upper=1> mu_ww = inv_logit(logit_mu_ww);  // back-transform to [0,1] scale
}
  
model {
  // Priors
  p ~ beta(${alpha_p}, ${beta_p});
  logit_mu_1 ~ normal(${mean_logit_mu_1}, ${sd_logit_mu_1});
  phi_1 ~ gamma(${shape_phi_1}, ${rate_phi_1});
  logit_mu_ww ~ normal(${mean_logit_mu_ww}, ${sd_logit_mu_ww});
  phi_ww ~ gamma(${shape_phi_ww}, ${rate_phi_ww});  // Updated from placeholder
  
  // Marginalized likelihood for pooled testing data
  for (i in 1:N_pools) {
    vector[n_swabs[i] + 1] lpmf_sum;  // to store log probabilities for each possible n_pos value
    for (n_pos in 0:n_swabs[i]) {
      // Probability of n_pos positive samples out of n_swabs[i]
      real binomial_lpmf_val = binomial_lpmf(n_pos | n_swabs[i], p);
      
      // Probability of observing r_viral_pool[i] given n_pos positive samples
      real nb_lpmf_val; 
      if (n_pos == 0){
        // If no positive samples in the pool, we should see zero viral reads
        nb_lpmf_val = r_viral_pool[i] == 0 ? 0 : negative_infinity(); 
      } else {
        nb_lpmf_val = neg_binomial_2_lpmf(r_viral_pool[i] | r_total_pool[i] * n_pos * mu_1 / n_swabs[i], n_pos * phi_1);
      }
      
      // Store the sum of log probabilities
      lpmf_sum[n_pos + 1] = binomial_lpmf_val + nb_lpmf_val;
    }
    
    // Marginalize out n_pos[i] using log-sum-exp for numerical stability
    target += log_sum_exp(lpmf_sum);
  }
  
  // Wastewater MGS
  for (j in 1:N_ww) {
    real mu = r_total_ww[j] * p * mu_ww;
    r_viral_ww[j] ~ neg_binomial_2(mu, phi_ww);
  }
}
"
stan_model <- stan_model(model_code = str_interp(model_code))
```

# Fitting the model

## Data Loading and Preparation

Now we'll load and prepare the data for our analysis:

```{r}
#| code-fold: false
# Swab data

swab_metadata <- read_tsv("../../tables/swab-sample-metadata.tsv") %>%
  mutate(date = ymd(paste0("20", date))) 

# Use the per-day data. Caveat about the different treatments
swab_reads <- read_tsv("../../tables/swabs-ra-summary.tsv") %>%
  mutate(date = ymd(paste0("20", date))) %>%
  select(-all_reads)

# Wastewater data

wastewater_metadata <- read_tsv("../../tables/wastewater-sample-metadata.tsv") %>%
  mutate(date = ymd(paste0("20", date))) 

wastewater_reads <- read_tsv("../../tables/ww-ra-summary.tsv") %>%
  mutate(date = ymd(paste0("20", date))) %>%
  select(-all_reads)
```

Next, we'll organize the virus species data and prepare it for analysis:

```{r}
#| code-fold: false

swab_species <- swab_reads %>%
  select(species, group) %>%
  distinct

wastewater_species <- wastewater_reads %>%
  select(species, group) %>%
  distinct

all_species <- full_join(wastewater_species, swab_species)

species_order <- all_species %>%
  arrange(group, species) %>%
  pull(species)

# Fill in zero counts
swab_reads_complete <- crossing(all_species, swab_metadata) %>%
  left_join(
    summarise(swab_reads, .by = c(date, location, species, group), viral_reads = sum(dedup_hv)),
    by = join_by(date, location, species, group),
  ) %>%
  arrange(group, species) %>%
  mutate(
    viral_reads= replace_na(viral_reads, 0),
    species = fct_inorder(species),
    group = fct_inorder(group)
    )

wastewater_reads_complete <- wastewater_metadata %>%
  crossing(all_species) %>%
  left_join(
    summarise(wastewater_reads, .by = c(date, location, species, group), viral_reads = sum(dedup_hv)),
    by = join_by(date, location, species, group),
  ) %>%
  arrange(group, species) %>%
  mutate(
    viral_reads= replace_na(viral_reads, 0),
    species = fct_inorder(species),
    group = fct_inorder(group)
    )
```

This data preparation ensures we have complete data for all species across all samples, filling in zeros where necessary. We organize the data by virus species and group for easier analysis.

## Model Fitting

Now we'll assemble the complete dataset and fit the model to each virus species:

```{r}
#| label: fit-model
#| cache: true
#| code-fold: false

species_data <- full_join(
    swab_reads_complete %>% nest(.by = c(species, group)),
    wastewater_reads_complete %>% nest(.by = c(species, group)),
    by = join_by(species, group),
    suffix = c(".swab", ".wastewater")
)

create_stan_data <- function(df.sw, df.ww) {
  list(
    N_pools = nrow(df.sw),
    n_swabs = df.sw$pool_size,
    r_total_pool = df.sw$all_reads,
    r_viral_pool = df.sw$viral_reads,
    N_ww = nrow(df.ww),
    r_total_ww = df.ww$all_reads,
    r_viral_ww = df.ww$viral_reads
  )
}

fit_model <- function(d) {
  sampling(stan_model, d, iter = 4000, warmup = 1000, chains = 4, cores = 4, seed = 381928)
}

fits <- species_data %>%
  mutate(
    stan_data = map2(data.swab, data.wastewater, create_stan_data),
    fit = map(stan_data, fit_model)
  )
```

For model fitting, we use 4 chains run for 4,000 iterations each and a warmup of 1,000 iterations.

## Model Diagnostics

After fitting the model, it's important to check diagnostics to ensure the model is reliable.
These diagnostics help us assess the quality of the model fits:

- Divergent transitions indicate potential issues with the model
- Maximum treedepth indicates how complex the posterior exploration was
- Rhat values close to 1 indicate good convergence

```{r}
#| code-fold: false

get_diagnostics <- function(f) {
  check <- summary(f)$summary
  sampler_params <- get_sampler_params(f, inc_warmup = FALSE)
  
  n_divergent <- sum(do.call(rbind, sampler_params)[, "divergent__"])
  max_treedepth <- max(do.call(rbind, sampler_params)[, "treedepth__"])
  n_max_treedepth <- sum(do.call(rbind, sampler_params)[, "treedepth__"] == 10)  # Assuming max_treedepth of 10
  rhat_max <- max(check[, "Rhat"], na.rm = TRUE)
  
  list(
    n_divergent = n_divergent,
    max_treedepth = max_treedepth,
    n_max_treedepth = n_max_treedepth,
    rhat_max = rhat_max
  )
}

fits %>%
  mutate(diagnostics = map(fit, get_diagnostics)) %>%
  select(species, group, diagnostics) %>%
  unnest_wider(diagnostics) %>%
  kable
```

The only warning sign here is that we have 5 divergent transitions for `HMPV-`.
However, this is one of the viruses with zero observations in the pooled swab data, so I'm not too concerned about this.

We extract the posterior samples for further analysis, and write to text for later:

```{r}
#| code-fold: false
posteriors <- fits %>%
  mutate(posterior = map(fit, compose(as_tibble, extract))) %>%
  select(species, group, posterior) %>%
  unnest(cols = posterior) %>%
  mutate(
    across(-c(species, group), as.double),
    )

write_tsv(posteriors, "posteriors.tsv")
```

# Posterior distributions

## Comparing Model Estimates with Naive Estimates

We can compare our model-based prevalence estimates with a naive estimate: the proportion of positive pools.

```{r}
left_join(
  swab_reads_complete %>%
    summarise(
      pos_pools_div_by_total_swabs = sum(viral_reads > 0) / sum(pool_size),
      .by = c(species, group),
      ),
  posteriors %>%
    summarise(
      posterior_median = median(p),
      .by = species,
    )
  ) %>%
  mutate(ratio = posterior_median / pos_pools_div_by_total_swabs) %>%
  kable
```

The posterior medians are higher by a factor usually between 1 and 2.
This makes sense since the naive estimate must be a lower bound on the prevalence.

Small ratios here suggest that the model is not making any wild extrapolations.
Conditional on the model and the priors, it is more likely that we're observing most of the positive cases than that the swab viral content is very low while the prevalence is high.

## Marginal Posterior Distributions

Let's visualize the posterior distributions for our key parameters.
In the following plots, solid lines show the posterior distribution of prevalence for each virus species, compared to the prior distribution (dashed line).
Colors represent viral groups (SARS-CoV-2, seasonal Coronaviruses, Influenza, Mononegavirales, and Rhinoviruses).

For reference, here's a summary of the data:

```{r}
full_join(
  swab_reads_complete %>%
    summarise(
      `+ pools` = sum(viral_reads > 0),
      `swab reads` = sum(viral_reads),
      .by = species,
    ),
  wastewater_reads_complete %>%
    summarise(
      `+ ww samples` = sum(viral_reads > 0),
      `ww reads` = sum(viral_reads),
      .by = species,
    ),
  by = join_by(species)
  ) %>%
  kable
```

### Prevalence

```{r}
prior_density <- function(x, density_function, log10x = FALSE){
  if (log10x){
    data <- tibble(
      x = x,
      density = density_function(x) * x * log(10), # Jacobian adjustment
    )
  } else {
    data <- tibble(
      x = x,
      density = density_function(x),
    )
  }
  geom_line(
    aes(x, density),
    data = data,
    color="grey",
    linetype="dashed",
    inherit.aes = FALSE
    )
}

scientific <- function(x) parse(text = sprintf("10^{%d}", log10(x)))
```

```{r}
posteriors %>%
  ggplot(aes(p, color=group)) +
  geom_density() +
  prior_density(10^seq(-6, 0, length.out=100), prior_p, log10x = TRUE) +
  scale_x_log10(limits = c(1e-7, 1), labels = scientific) +
  facet_wrap(vars(species), ncol = 4) +
  labs(x = "prevalence", y = "") +
  theme(legend.position = "", axis.text.y = element_blank()) 
```

This plot shows that under our model, the swab data can place an upper bound on the prevalence.
The right tails of the posterior distributions fall off faster than the prior.

However, it's hard to see whether the lower bound is driven more by the prior or the posterior.
To remedy this, we can plot the density on a log scale:

```{r}
posteriors %>%
  ggplot(aes(p, color=group)) +
  geom_density() +
  prior_density(10^seq(-8, 0, length.out=100), prior_p, log10x = TRUE) +
  scale_x_log10(limits = c(1e-7, 1), labels = scientific) +
  facet_wrap(vars(species), ncol = 4) +
  scale_y_continuous(transform = "log10", limits = c(1e-3, 10), breaks = 10^c(0, -2), labels = scientific) +
  labs(x = "prevalence", y = "") +
  theme(legend.position = "") #, axis.text.y = element_blank()) 
```

On a log scale, it's clear that the left tail is also being constrained by the data rather than the prior.
For the viruses that appear in the swab samples (all except Flu B, H3N2, HMPV-1, HPIV1, and HPIV2), the cutoff happens around the order of 1/1,000, which makes sense since there are 547 total swabs.

For viruses that only appear in the wastewater data, the posterior follows the prior more closely,
though shifted to the left (because the lack of swab observations precludes high prevalence).
There may be signs that the left tail is constrained, especially for H3N2, perhaps because of the observations in the wastewater data. This may just be due to the difficulty of sampling the extreme tails of the distribution.

### Expected viral fractions

Posteriors on $\mu_1$ (swab viral fraction with all positive swabs).

With linear y-scale:

```{r}
breaks <- 10^seq(-5, -1, 2)
posteriors %>%
  ggplot(aes(logit_mu_1, color=group)) +
  geom_density() +
  prior_density(seq(-16, 2, length.out=100), prior_logit_mu_1) +
  scale_x_continuous(limits = c(-14, 2), breaks = qlogis(breaks), labels = scientific(breaks)) +
  facet_wrap(vars(species), ncol = 4) +
  labs(y = "", x = "mu_1") +
  theme(legend.position = "", axis.text.y = element_blank()) 
```

And log y-scale:

```{r}
posteriors %>%
  ggplot(aes(logit_mu_1, color=group)) +
  geom_density() +
  prior_density(seq(-16, 2, length.out=100), prior_logit_mu_1) +
  scale_x_continuous(limits = c(-14, 2), breaks = qlogis(breaks), labels = scientific(breaks)) +
  facet_wrap(vars(species), ncol = 4) +
  scale_y_continuous(transform = "log10", limits = c(1e-3, 1)) +
  labs(y = "", x = "mu_1") +
  theme(legend.position = "")#  , axis.text.y = element_blank()) 
```

For species with multiple positive pools, (e.g. the Rhinoviruses), there is information to constrain the expected viral fraction in a positive swab.
With only one positive pool (e.g. H1N1, which has only one viral read), there is less, but still some information.
For the unobserved viruses, the posterior just follows the prior.
We should therefore think about how much we trust this prior before believing the estimates for these latter viruses.

HCoV-229E is an outlier because it has only a single positive pool with 6008 viral reads.
Here, the prior is significantly constraining the posterior, which with a flatter prior would put more weight on $\mu \sim 1$.
I think this is probably reasonable, but we should think more about that.


Same plots but for $\mu_{ww}$:

```{r}
breaks <- 10^seq(-7, -3, 2)
posteriors %>%
  ggplot(aes(logit_mu_ww, color=group)) +
  geom_density() +
  prior_density(seq(-18, -5, length.out=100), prior_logit_mu_ww) +
  scale_x_continuous(limits = c(-18, -5), breaks = qlogis(breaks), labels = scientific(breaks)) +
  scale_y_continuous(name = "") +
  facet_wrap(vars(species), ncol = 4) +
  labs(y = "", x = "mu_ww") +
  theme(legend.position = "", axis.text.y = element_blank()) 
```

```{r}
posteriors %>%
  ggplot(aes(logit_mu_ww, color=group)) +
  geom_density() +
  prior_density(seq(-18, -5, length.out=100), prior_logit_mu_ww) +
  scale_x_continuous(limits = c(-18, -5), breaks = qlogis(breaks), labels = scientific(breaks)) +
  facet_wrap(vars(species), ncol = 4) +
  scale_y_continuous(transform = "log10", limits = c(1e-3, 1)) +
  labs(y = "", x = "mu_ww") +
  theme(legend.position = "")
```

Here, the prior only seems to have significant influence on the viruses that are unobserved in the swab samples (and thus have weak prevalence estimates).

### Overdispersion

Posteriors for swab inverse-overdispersion $\phi_1$.

```{r}
posteriors %>%
  ggplot(aes(phi_1, color=group)) +
  geom_density() +
  prior_density(10^seq(-3, 3, length.out=100), prior_phi_1, log10x = TRUE) +
  scale_x_log10() +
  facet_wrap(vars(species), ncol = 4) +
  labs(y = "") +
  theme(legend.position = "", axis.text.y = element_blank())
```
Here, we have almost no information in the data, which is expected.
The two cases where the posterior is different from the prior have 4 or 5 positive pools. Interestingly, so does Rhinovirus C, which just recapitulates the prior. 

Posteriors for wastewater inverse-overdispersion $\phi_ww$.

```{r}
posteriors %>%
  ggplot(aes(phi_ww, color=group)) +
  geom_density() +
  prior_density(10^seq(-3, 3, length.out=100), prior_phi_ww, log10x = TRUE) +
  scale_x_log10(limits = c(1e-3, 1e3)) +
  facet_wrap(vars(species), ncol = 4) +
  labs(y = "") +
  theme(legend.position = "", axis.text.y = element_blank())
```

The wastewater data has more non-zero counts and thus more data to infer dispersion.
I'm not sure I trust any of these though.

Generally, the species that have high predicted overdispersion relative to the prior (low $\phi$) are ones with fewer positive wastewater samples. 
That could be because there are more zero-read samples than expected given the counts in the non-zero-read samples.

Something funny is going on with HPIV1, HPIV2, and HMPV-1 which are unobserved in the swabs and only observed in 1-3 the wastewater samples.
It's probably worth following up on these, unless we want to throw out the unobserved swab viruses altogether.

Another option would be to use a combined model where all the viruses share (totally or hierarchically) $\phi$ parameters. This could make sense since lots of the sources of overdispersion are probably shared between viruses.
If true, this would allow us to get better estimates of $\phi$.

## Joint Posterior Distributions

We can also examine relationships between different parameters in the posterior.
In the following plots, each point is a sample from the joint posterior of two parameters.
The curves are an estimate of the conditional mean of the y-axis parameter given the x-axis parameter.

### Realtionships with prevalence

Prevalence vs $\mu_1$ (note log x-scale not logit):

```{r}
posteriors %>%
  ggplot(aes(mu_1, p)) +
  geom_point(alpha = 0.05, size = 0.1) +
  geom_smooth() +
  scale_x_log10(limits = c(1e-7, 1), labels = scientific) +
  scale_y_log10(limits = c(1e-6, 1), labels = scientific) +
  facet_wrap(~species, ncol=6)
```

It looks like:

- For small $\mu$, prevalence grows as $\mu$ decreases.
- For larger $\mu$, this relationship flattens.

For observed viruses, this makes sense because low $\mu$ requires high prevalence to observe the virus since each positive swab is likely to go unobserved, while with high $\mu$ positive swabs are usually observed, so the prevalence is determined by the number of positive pools.

For unobserved viruses, low $\mu$ means explains the lack of reads by itself so $p$ reverts to its prior.
High $\mu$ requires low $p$ to explain the lack of reads, but it only has to be low enough for that $n_pos$ is likely to be zero.
Since the total swabs is ~500, the distribution of $p$ is capped at $\sim 1/500$ for high $\mu$.

```{r}
posteriors %>%
  ggplot(aes(phi_1, p)) +
  geom_point(alpha = 0.05, size = 0.1) +
  geom_smooth() +
  scale_x_log10(limits = c(1e-2, 10), labels=scientific) +
  scale_y_log10(limits = c(1e-6, 1), labels = scientific) +
  facet_wrap(~species, ncol = 6)
```

There appears to be a relationship between the swab inverse overdispersion and the prevalence estimates, suggesting that if we used a prior that allowed smaller $\phi_1$, we might get larger prevalence estimates for some viruses.

This makes sense because if we only see a virus a few times and overdispersion is very large, the true prevalence might be higher than we think but we have a lot of false negatives and our observations are lucky outliers.

We should consider whether we like the current prior on $\phi_1$ or want to allow it to be broader.

On the other hand, the relationship is pretty weak compared to the spread of the $p$ samples, so it may not matter very much.

### Relationships with expected wastewater viral fraction

Prevalence: 

```{r}
posteriors %>%
  ggplot(aes(p, mu_ww)) +
  geom_point(alpha = 0.05, size = 0.1) +
  geom_smooth() +
  scale_x_log10(labels = scientific) +
  scale_y_log10(labels = scientific) +
  facet_wrap(~species, ncol = 6)
```

As you might expect, there is a very tight relationship between prevalence and estimated wastewater viral fraction (per prevalence).
Often, strong correlations between parameters suggest that we should reparameterize the model so the parameters are more independent.
However, in this case, I think this is appropriate. We need a distinct $p$ parameter to make use of the discreteness of the swab pools.

The next three plots show the relationship between $\mu_{ww}$ and $\mu_1$, $\phi_1$, and $\phi_{ww}$:

```{r}
posteriors %>%
  ggplot(aes(mu_1, mu_ww)) +
  geom_point(alpha = 0.05, size = 0.1) +
  geom_smooth() +
  scale_x_log10(labels = scientific) +
  scale_y_log10(labels = scientific) +
  facet_wrap(~species, ncol=6)
```


```{r}
posteriors %>%
  ggplot(aes(phi_1, mu_ww)) +
  geom_point(alpha = 0.05, size = 0.1) +
  geom_smooth() +
  scale_x_log10(labels = scientific) +
  scale_y_log10(labels = scientific) +
  facet_wrap(~species, ncol=6)
```

```{r}
posteriors %>%
  ggplot(aes(phi_ww, mu_ww)) +
  geom_point(alpha = 0.05, size = 0.1) +
  geom_smooth() +
  scale_x_log10(label = scientific) +
  scale_y_log10(label = scientific) +
  facet_wrap(~species)
```

The relationships are all pretty weak, especially between $\mu_{ww}$ and the $\phi$ parameters.
This is encouraging because it suggests that if our primary goal is to estimate $\mu_{ww}$, we don't have to worry too much about the accuracy of our other inferences. This is especially important for the $\phi$ parameters where the data does not constrain our estimates very much.
(Caveat that this weak relationship may be an artefact of artifically constraining priors on $\phi$. Maybe if we used a very broad prior we'd find a stronger relationship with $\mu_{ww} driven by extreme values.)

## Posterior Predictive Checks

### Swab data

To validate our model, we can use posterior predictive checks - generating new data from the fitted model and comparing it to the observed data.
The following table gives the summary statistics for the data.

```{r}
#| label: swab-ppd
#| cache: true
# Posterior predictive distributions

## Swabs 

posterior_predictive_swab <- posteriors %>%
  mutate(rep = 1:n(), .by=species) %>%
  cross_join(swab_metadata) %>%
  mutate(
    n_pos = map2_int(pool_size, p, ~ rbinom(1, .x, .y)),
    mu = n_pos * mu_1 * all_reads / pool_size,
    phi = n_pos * phi_1,
    viral_reads = map2_int(mu, phi, ~ifelse(.x > 0, rnbinom(1, size = .y, mu = .x), 0)),
    false_negative = n_pos > 0 & viral_reads == 0,
  )

posterior_predictive_swab_summary <- posterior_predictive_swab %>%
  summarise(
    pos_pools = sum(viral_reads > 0),
    viral_reads_total = sum(viral_reads),
    viral_reads_max = max(viral_reads),
    viral_reads_sd = sd(viral_reads),
    false_negative_pools = sum(false_negative),
    across(c(p, mu_1, phi_1), first),
    .by = c(species, group, rep),
  )

species_swab_summary <- swab_reads_complete %>%
  summarise(
    pos_pools = sum(viral_reads > 0),
    viral_reads_total = sum(viral_reads),
    viral_reads_max = max(viral_reads),
    viral_reads_sd = sd(viral_reads),
    .by = c(species, group),
  ) %>%
  arrange(species)

kable(species_swab_summary)
```


```{r}
posterior_predictive_swab_summary %>%
  ggplot(aes(x = pos_pools, fill = group, after_stat(density))) +
  geom_histogram(binwidth = 1, center = 0, color = "grey") +
  geom_vline(data = species_swab_summary, mapping = aes(xintercept=pos_pools)) +
  facet_wrap(vars(species)) +
  scale_x_continuous(breaks = seq(0, 12, 2), name = "positive pools")
```

This plot compares the observed number of positive pools (vertical line) with the distribution of positive pools generated from our model.
For every virus, the observed number of positive pools is common in the simulations.

```{r}
posterior_predictive_swab_summary %>%
  ggplot(aes(x = viral_reads_total, y = species, fill = group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_point(data = species_swab_summary) +
  scale_x_log10(
    breaks = 10^seq(0, 4),
    name = "total viral read count (conditional on > 0)",
    oob = scales::oob_keep,
  ) +
  scale_y_discrete(limits = rev(species_order))
```

This plot compares the observed total viral reads (points) with the distribution from our model.
Violins show only the simulations with at least one viral read.
Vertical bars show quartiles (conditional on at least one read).


```{r}
posterior_predictive_swab_summary %>%
  ggplot(aes(x = viral_reads_max, y = species, fill = group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_point(data = species_swab_summary) +
  scale_x_log10(
    breaks = 10^seq(0, 4),
    name = "largest viral read count (conditional on > 0)",
    oob = scales::oob_keep,
  ) +
  scale_y_discrete(limits = rev(species_order))
```

This plot compares the observed maximum viral reads with the distribution from our model.
Violins show only the simulations with at least one viral read.
Vertical bars show quartiles (conditional on at least one read).

```{r}
posterior_predictive_swab_summary %>%
  ggplot(aes(x = viral_reads_sd, y = species, fill = group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_point(data = species_swab_summary) +
  scale_x_log10(
    breaks = 10^seq(0, 4),
    name = "stdev viral read count (conditional on > 0)",
    oob = scales::oob_keep,
  ) +
  scale_y_discrete(limits = rev(species_order))
```

This plot compares the observed standard deviation of viral reads with the distribution from our model.
Violins show only the simulations with at least one viral read.
Vertical bars show quartiles (conditional on at least one read).

Together, these plots suggest that the model is capturing the swab data reasonably well.

### Wastewater data

Now we'll perform similar posterior predictive checks for the wastewater data:

```{r}
#| label: wastewater-ppd
#| cache: true
## Wastewater

posterior_predictive_ww <- posteriors %>%
  mutate(rep = 1:n(), .by=species) %>%
  cross_join(wastewater_metadata) %>%
  mutate(
    mu = p * mu_ww * all_reads,
    viral_reads = map2_int(mu, phi_ww, ~ifelse(.x > 0, rnbinom(1, size = .y, mu = .x), 0)),
  )

posterior_predictive_ww_summary <- posterior_predictive_ww %>%
  summarise(
    pos_samples = sum(viral_reads > 0),
    viral_reads_total = sum(viral_reads),
    viral_reads_max = max(viral_reads),
    viral_reads_sd = sd(viral_reads),
    across(c(p, mu_ww, phi_ww), first),
    viral_reads_expected = p * mu_ww * sum(all_reads),
    .by = c(species, group, rep),
  )

species_ww_summary <- wastewater_reads_complete %>%
  summarise(
    pos_samples = sum(viral_reads > 0),
    viral_reads_total = sum(viral_reads),
    viral_reads_max = max(viral_reads),
    viral_reads_sd = sd(viral_reads),
    .by = c(species, group),
  ) %>%
  arrange(species)

kable(species_ww_summary)
```

This code calculates summary statistics for the observed wastewater data, which we'll use to compare with predictions from our model.

```{r}
posterior_predictive_ww_summary %>%
  ggplot(aes(x = pos_samples, fill = group, after_stat(density))) +
  geom_histogram(binwidth = 1, center = 0, color = "grey") +
  geom_vline(data = species_ww_summary, mapping = aes(xintercept = pos_samples)) +
  facet_wrap(vars(species)) +
  scale_x_continuous(breaks = seq(0, 12, 2), name = "positive samples")
```

This plot compares the observed number of positive wastewater samples (vertical line) with the distribution generated from our model.
It looks like the model systematically overestimates the number of wastewater samples with viral reads (though not by a lot in any one case).
This may be a sign that $\phi_{ww}$ is too large: too little overdispersion leading to too few zero counts given a mean relative abundance.
Or it could be that $\mu_{ww}$ is too slightly high, perhaps because of the prior.

```{r}
posterior_predictive_ww_summary %>%
  ggplot(aes(x = viral_reads_total, y = species, fill = group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_point(data = species_ww_summary) +
  scale_x_log10(
    breaks = 10^seq(0, 4),
    limits = c(1, 2e4),
    # The log scale implicitly censors zero counts, so the violins 
    # are only calculated from the non-zero counts
    name = "total viral read count (conditional on > 0)",
    oob = scales::oob_keep,
  ) +
  scale_y_discrete(limits = rev(species_order))
```

This plot compares the observed total viral reads in wastewater (points) with the distribution predicted by our model.
The observations here are all very close to the posterior median.
I'm not sure if this is a problem or not.
I'm inclined to think that it's not, given that the posteriors are widely dispersed.

```{r}
posterior_predictive_ww_summary %>%
  ggplot(aes(x = viral_reads_max, y = species, fill = group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_point(data = species_ww_summary) +
  scale_x_log10(
    breaks = 10^seq(0, 4),
    limits = c(1, 2e4),
    name = "largest viral read count (conditional on > 0)",
    oob = scales::oob_keep,
  ) +
  scale_y_discrete(limits = rev(species_order))
```

This plot compares the observed maximum viral reads in wastewater with the distribution from our model.

Here the model might be systematically slightly overestimating the maximum viral reads.
This cuts against the "too little overdispersion" hypothesis for the excess positive samples.
Perhaps $\mu_{ww}$ is slightly too high, perhaps because the prior is pushing the estimates up?

Reviewing the posteriors by eye, roughly 13/17 shift left compared to the prior, only 2/17 shift right.
If there's a subtle bias on a logit scale it might be enough to explain the deficit of zeros, which is noticeable but not large and the bias in the posterior predictive count stats, which is small.

```{r}
posterior_predictive_ww_summary %>%
  ggplot(aes(x = viral_reads_sd, y = species, fill = group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_point(data = species_ww_summary) +
  scale_x_log10(
    breaks = 10^seq(0, 4),
    name = "stdev viral read count (conditional on > 0)",
    oob = scales::oob_keep,
  ) +
  scale_y_discrete(limits = rev(species_order))
```

This plot compares the observed standard deviation of viral reads in wastewater with the distribution from our model.

Here the model might also be systematically slightly too high.
This is more support for the $\mu_{ww}$ too high explanation
because higher mean count implies higher standard deviation in the negative binomial model.

Another interpretation is too much overdispersion, which would account for high standard deviation and high maximum count.

## Computing RA_p(1%) Estimates

Finally, we can calculate the RA_p(1%) values - the relative abundance at 1% prevalence - which was a key goal of the original analysis:

```{r}
# RA_i(1%) and related quantities

posteriors %>%
  mutate(ra01 = mu_ww * 0.01) %>%
  ggplot(aes(ra01, species, fill=group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  scale_x_log10(name = "wastewater RA_p(1%)") +
  scale_y_discrete(limits=rev(species_order))
```

This plot shows the posterior distribution of RA_p(1%) for each virus species.
Vertical lines represent quartiles of the posterior.

```{r}
posteriors %>%
  mutate(ratio = mu_1 / mu_ww) %>%
  ggplot(aes(ratio, species, fill=group)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  scale_x_log10(name = "swab RA / wastewater RA", limits=c(1e-2,1e8), breaks=10^seq(0,6,2)) +
  scale_y_discrete(limits=rev(species_order))
```

This plot shows the ratio of swab relative abundance to wastewater relative abundance for a fixed prevalence.
Vertical lines represent quartiles of the posterior.

One striking result here is that the posteriors put significant weight (>25%) on
the possibility that H3N2 has higher RA in wastewater than in swabs.
Given the data that we have, this seems plausible: it appears in all the wastewater samples and none of the swabs
and has the fourth-highest total read count in wastewater.
On the other hand, maybe this suggests a bioinformatics issue identifying H3N2 in the ONT data.

```{r}
posteriors %>%
  mutate(ra01 = mu_ww * 0.01) %>%
  summarise(
    q25 = quantile(ra01, 0.25),
    median = median(ra01),
    q75 = quantile(ra01, 0.75),
    interquartile_ratio = format(q75 / q25, digits = 2),
    .by = c(species, group)
  ) %>%
  mutate(across(-c(species, group, interquartile_ratio), ~format(.x, scientific=TRUE, digits = 2))) %>%
  kable
```

This table provides quantiles of the RA_p(1%) estimates for each virus species, representing the uncertainty in these values.

```{r}
posteriors %>%
  mutate(ratio = mu_1 / mu_ww) %>%
  summarise(
    q25 = quantile(ratio, 0.25),
    median = median(ratio),
    q75 = quantile(ratio, 0.75),
    .by = c(species, group)
  ) %>%
  mutate(across(-c(species, group), ~format(.x, scientific=TRUE, digits = 2))) %>%
  kable
```

This table provides quantiles of the ratio of swab to wastewater sensitivity for each virus species.

# Conclusions

1. Overall, the model fits looks reasonable to me and the posterior predictive checks look good.
2. I would put least faith on the estimates where we don't observe the virus in the swabs. However, I think there is some information here since the swabs put an upper bound on the prevalence, which becomes a lower bound on RA_p(1%).
3. Posterior predictive checks suggest that we may want to broaden the prior on $\mu_{ww}$ or shift it to the left a bit.
4. We might gain more information about the overdispersion ($\phi$) parameters with a hierarchical model that uses information across viruses to estimate the overdispersion. However, it's not clear that doing so will give us better estimates of RA_p.
A potentially serious difficulty with implementing a hierarchical model is that you'd have to sum over all combinations of n_pos for each virus. The size of this calculation blows up exponentially in the number of viruses. There's probably some trick to be done (maybe there's smarter algorithm, maybe we can use the fact that most of the pools are negative and the prevalences are small to constrain the sum to the corner where the n_pos are small?) but it might be fiddly.

I suspect this is not worth trying to implement for this project.

## Other things we might want to check

- Is there any information transfer from wastewater to swabs? How much?
- Log-normal poisson read counts to model multiplicative sources of noise
- Sensitivity to prior on $p$ especially since all the posteriors are shifted left relative to the prior
- 

## Claude's suggestions:

### Statistical interpretation

1. Parameter Identification: There's some discussion about the correlation between parameters (e.g., prevalence and mu_ww), but not much about potential identifiability issues. Given that some viruses have few or no observations in the swab data, it would be worth discussing the extent to which the model can distinguish between low prevalence and low detection sensitivity.
2. Unobserved Viruses: The author correctly notes lower confidence in estimates for viruses unobserved in swabs, but might want to more clearly state the limitations of inference in these cases.
3. HCoV-229E Outlier: The author notes this virus has a single positive pool with 6008 viral reads, which is dramatically higher than others. It would be worth discussing if this could be a technical artifact or lab contamination rather than a biological difference.

### Posterior Predictive Checks

1. The wastewater PPCs show some systematic deviations (overestimation of positive samples, slight overestimation of counts). This might indicate model misspecification rather than just parameter misspecification.
2. The model seems to be systematically overestimating the number of positive wastewater samples. The author attributes this to either phi_ww being too large or mu_ww being too high, but doesn't consider the possibility that the relationship between prevalence and wastewater detection might be non-linear or require additional covariates.
[EF note: this change would make the parameters less interpretable and probably isn't worth it for slightly better posterior predictive checks.]

### Potential improvements 

1. Sensitivity Analysis: Consider analyzing how sensitive the RA_p(1%) estimates are to the choice of priors, especially for viruses with limited data.
2. Model Comparison: It would be valuable to formally compare this model to simpler alternatives (e.g., naive estimators) using metrics like WAIC or LOO-CV to quantify the improvement.
3. Hierarchical Priors: The author mentions considering a hierarchical model for phi parameters across viruses. This seems like a promising direction and could improve estimates, especially for viruses with limited data.
4. Alternative Models: For the wastewater component, where there seems to be some model misfit, it might be worth exploring alternative functional forms for the relationship between prevalence and expected viral abundance.
5. Posterior Predictive p-values: Consider calculating formal posterior predictive p-values to quantify the degree of model fit rather than relying solely on visual assessment.
